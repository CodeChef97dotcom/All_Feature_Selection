{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math,time,sys\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import seaborn as sns \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.naive_bayes import \n",
    "# from sklearn.ensemble import RanfomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "MaxIter = 1\n",
    "pop_size = 4\n",
    "omega = 0.99\n",
    "bp = 0.5\n",
    "bsize = 1\n",
    "wsize = 3\n",
    "\n",
    "def initialise(partCount, dim, trainX, testX, trainy, testy):    \n",
    "    population=np.zeros((partCount,dim))\n",
    "    minn = 1\n",
    "    maxx = math.floor(0.5*dim)\n",
    "    \n",
    "    if maxx<minn:\n",
    "        maxx = minn + 1\n",
    "        #not(c[i].all())\n",
    "    \n",
    "    for i in range(partCount):\n",
    "        random.seed(i**3 + 10 + time.time() ) \n",
    "        no = random.randint(minn,maxx)\n",
    "        if no == 0:\n",
    "            no = 1\n",
    "        random.seed(time.time()+ 100)\n",
    "        pos = random.sample(range(0,dim-1),no)\n",
    "        for j in pos:\n",
    "            population[i][j]=1\n",
    "            \n",
    "    return population\n",
    "\n",
    "def fitness(agent, trainX, testX, trainy, testy):\n",
    "    # print(agent)\n",
    "    cols=np.flatnonzero(agent)\n",
    "    # print(cols)\n",
    "    val=1\n",
    "    if np.shape(cols)[0]==0:\n",
    "        return val    \n",
    "    clf=RandomForestClassifier(n_estimators=20)\n",
    "    #clf=MLPClassifier(alpha=0.001, hidden_layer_sizes=(1000,500,100),max_iter=2000,random_state=4)\n",
    "    train_data=trainX[:,cols]\n",
    "    test_data=testX[:,cols]\n",
    "    clf.fit(train_data,trainy)\n",
    "    val=1-clf.score(test_data,testy)\n",
    "\n",
    "    #in case of multi objective  []\n",
    "    set_cnt=sum(agent)\n",
    "    set_cnt=set_cnt/np.shape(agent)[0]\n",
    "    val=omega*val+(1-omega)*set_cnt\n",
    "    return val\n",
    "\n",
    "def allfit(population, trainX, testX, trainy, testy):\n",
    "    x=np.shape(population)[0]\n",
    "    acc=np.zeros(x)\n",
    "    for i in range(x):\n",
    "        acc[i]=fitness(population[i],trainX, testX, trainy, testy)     \n",
    "        #print(acc[i])\n",
    "    return acc\n",
    "\n",
    "def test_accuracy(agent, trainX, testX, trainy, testy):\n",
    "    cols=np.flatnonzero(agent)\n",
    "    val=1\n",
    "    if np.shape(cols)[0]==0:\n",
    "        return val    \n",
    "    # clf = RandomForestClassifier(n_estimators=300)\n",
    "    #clf=MLPClassifier(alpha=0.001, hidden_layer_sizes=(1000,500,100),max_iter=2000,random_state=4)\n",
    "    clf=KNeighborsClassifier(n_neighbors=5)\n",
    "    # clf=MLPClassifier( alpha=0.01, max_iterno=1000) #hidden_layer_sizes=(1000,500,100)\n",
    "    #cross=4\n",
    "    #test_size=(1/cross)\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(trainX, trainy,  stratify=trainy,test_size=test_size)\n",
    "    train_data=trainX[:,cols]\n",
    "    test_data=testX[:,cols]\n",
    "    clf.fit(train_data,trainy)\n",
    "    val=clf.score(test_data,testy)\n",
    "    return val\n",
    "\n",
    "def onecnt(agent):\n",
    "    return sum(agent)\n",
    "\n",
    "def sigmoid(gamma):\n",
    "    if gamma < 0:\n",
    "        return 1 - 1/(1 + math.exp(gamma))\n",
    "    else:\n",
    "        return 1/(1 + math.exp(-gamma))\n",
    "    \n",
    "def NMRA(dataset):\n",
    "    df = pd.read_csv(dataset)\n",
    "    a, b = np.shape(df)\n",
    "    data = df.values[:,0:b-1]\n",
    "    label = df.values[:,b-1]\n",
    "    dimension = data.shape[1]\n",
    "    \n",
    "    cross = 5\n",
    "    test_size = (1/cross)\n",
    "    trainX, testX, trainy, testy = train_test_split(data, label,stratify=label ,test_size=test_size,random_state=(7+17*int(time.time()%1000)))\n",
    "\n",
    "    clf=RandomForestClassifier(n_estimators=20)\n",
    "    clf.fit(trainX,trainy)\n",
    "\n",
    "    val=clf.score(testX,testy)\n",
    "    whole_accuracy = val\n",
    "    print(\"Total Acc: \",val)\n",
    "    \n",
    "    pop = initialise(pop_size, dimension, trainX, testX, trainy, testy)\n",
    "    fit = allfit(pop, trainX, testX, trainy, testy)\n",
    "    \n",
    "    ind = np.argsort(fit)\n",
    "    index_b, index_w = [], []\n",
    "\n",
    "    for i in range(bsize):\n",
    "        index_b.append(ind[i])\n",
    "        \n",
    "    for i in range(wsize):\n",
    "        index_w.append(ind[i])\n",
    "    \n",
    "    bestpop = pop[ind[0]]\n",
    "    bestfit = fit[ind[0]]\n",
    "    \n",
    "    for i in range(MaxIter):\n",
    "        for j in range(len(index_w)):\n",
    "            l = []\n",
    "            for k in range(dimension):\n",
    "                random.seed(time.time()+k*2)\n",
    "                l.append(random.random())\n",
    "                \n",
    "            pos = random.sample(range(0,len(index_w)-1),2)\n",
    "            s = np.add(pop[index_w[j]], np.multiply(l, np.subtract(pop[index_w[pos[0]]],pop[index_w[pos[1]]])))\n",
    "            \n",
    "            if(fitness(s,trainX, testX, trainy, testy) < fit[index_w[j]]):\n",
    "                fit[index_w[j]] = fitness(s,trainX, testX, trainy, testy)\n",
    "                pop[index_w[j]] = s.copy()\n",
    "                \n",
    "                \n",
    "        for j in range(len(index_b)):\n",
    "            random.seed(time.time())\n",
    "            if(random.random() < bp): \n",
    "                l, nl = [], []\n",
    "                for k in range(dimension):\n",
    "                    random.seed(time.time()+k*2)\n",
    "                    l.append(random.random())\n",
    "\n",
    "                for k in l:\n",
    "                    nl.append(1-k)\n",
    "\n",
    "                s = np.add(np.multiply(nl, pop[index_b[j]]), np.multiply(l, (np.subtract(bestpop, pop[index_b[j]]))))\n",
    "\n",
    "                if(fitness(s,trainX, testX, trainy, testy) < fit[index_b[j]]):\n",
    "                    fit[index_b[j]] = fitness(s,trainX, testX, trainy, testy)\n",
    "                    pop[index_b[j]] = s.copy()  \n",
    "                    \n",
    "        for j in range(pop_size):\n",
    "            for k in range(dimension):\n",
    "                random.seed(time.time())\n",
    "                if (sigmoid(pop[j][k]) > 0.5):\n",
    "                    pop[j][k] = 1\n",
    "                    \n",
    "                else:\n",
    "                    pop[j][k] = 0\n",
    "                \n",
    "                \n",
    "        fit = allfit(pop, trainX, testX, trainy, testy)\n",
    "        ind = np.argsort(fit)\n",
    "        index_b, index_w = [], []\n",
    "\n",
    "        for i in range(bsize):\n",
    "            index_b.append(ind[i])\n",
    "\n",
    "        for i in range(wsize):\n",
    "            index_w.append(ind[i])\n",
    "\n",
    "        bestpop = pop[ind[0]]\n",
    "        bestfit = fit[ind[0]]\n",
    "    \n",
    "    fit = allfit(pop, trainX, testX, trainy, testy)\n",
    "    ind = np.argsort(fit)\n",
    "    bestpop = pop[ind[0]]\n",
    "    bestfit = fit[ind[0]]\n",
    "    \n",
    "    testAcc = test_accuracy(bestpop, trainX, testX, trainy, testy)\n",
    "    featCnt = onecnt(bestpop)\n",
    "    #print(\"best agent: \", bestpop)\n",
    "    print(\"Test Accuracy: \", testAcc)\n",
    "    print(\"#Features: \", featCnt)\n",
    "            \n",
    "    return testAcc, featCnt, bestpop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Acc:  0.9777777777777777\n",
      "Test Accuracy:  0.9\n",
      "#Features:  36.0\n",
      "CPU times: user 1.16 s, sys: 19.1 ms, total: 1.18 s\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Ta,fc,bp = NMRA('VoxForge512.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004380</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.503850</td>\n",
       "      <td>0.436740</td>\n",
       "      <td>11.858993</td>\n",
       "      <td>8.202133</td>\n",
       "      <td>0.974210</td>\n",
       "      <td>2.181164</td>\n",
       "      <td>15.391540</td>\n",
       "      <td>11.462402</td>\n",
       "      <td>0.016049</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548493</td>\n",
       "      <td>0.212006</td>\n",
       "      <td>2.378749</td>\n",
       "      <td>5.564082</td>\n",
       "      <td>3.520760</td>\n",
       "      <td>6.403028</td>\n",
       "      <td>7.948945</td>\n",
       "      <td>11.421647</td>\n",
       "      <td>0.014899</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452232</td>\n",
       "      <td>0.603910</td>\n",
       "      <td>1.479533</td>\n",
       "      <td>1.281132</td>\n",
       "      <td>2.430287</td>\n",
       "      <td>5.495424</td>\n",
       "      <td>8.274411</td>\n",
       "      <td>11.088083</td>\n",
       "      <td>0.014856</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008713</td>\n",
       "      <td>0.008530</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.005249</td>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>...</td>\n",
       "      <td>4.925777</td>\n",
       "      <td>0.588674</td>\n",
       "      <td>24.452608</td>\n",
       "      <td>27.830118</td>\n",
       "      <td>0.291790</td>\n",
       "      <td>0.230298</td>\n",
       "      <td>0.284989</td>\n",
       "      <td>9.626831</td>\n",
       "      <td>0.011871</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004527</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.001894</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774701</td>\n",
       "      <td>0.889606</td>\n",
       "      <td>15.672068</td>\n",
       "      <td>3.751125</td>\n",
       "      <td>0.687353</td>\n",
       "      <td>1.843793</td>\n",
       "      <td>5.087078</td>\n",
       "      <td>9.285885</td>\n",
       "      <td>0.011261</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.003282</td>\n",
       "      <td>0.014107</td>\n",
       "      <td>0.009643</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.027846</td>\n",
       "      <td>29.929813</td>\n",
       "      <td>0.032924</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.005478</td>\n",
       "      <td>0.008104</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>0.012645</td>\n",
       "      <td>9.986907</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.008245</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>0.002771</td>\n",
       "      <td>9.939436</td>\n",
       "      <td>0.006557</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.002047</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>9.965343</td>\n",
       "      <td>0.004320</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005562</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.022594</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>0.014489</td>\n",
       "      <td>30.011419</td>\n",
       "      <td>0.019132</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.004380  0.002665  0.000497  0.000497  0.000682  0.000609  0.001508   \n",
       "1    0.001584  0.002996  0.000250  0.000208  0.000248  0.000122  0.000278   \n",
       "2    0.001206  0.001195  0.000330  0.000584  0.000674  0.000366  0.000559   \n",
       "3    0.008713  0.008530  0.001264  0.001742  0.000852  0.001149  0.001520   \n",
       "4    0.004527  0.003138  0.000424  0.000823  0.000760  0.000430  0.001003   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "895  0.000559  0.000726  0.000355  0.000523  0.001019  0.000396  0.000470   \n",
       "896  0.000161  0.000092  0.000126  0.000170  0.000105  0.000142  0.000113   \n",
       "897  0.000040  0.000042  0.000036  0.000030  0.000055  0.000043  0.000037   \n",
       "898  0.000005  0.000008  0.000010  0.000010  0.000012  0.000017  0.000010   \n",
       "899  0.000198  0.000293  0.000128  0.000094  0.000080  0.000063  0.000140   \n",
       "\n",
       "            7         8         9  ...        27        28         29  \\\n",
       "0    0.002521  0.001729  0.001222  ...  0.503850  0.436740  11.858993   \n",
       "1    0.001349  0.000641  0.000549  ...  0.548493  0.212006   2.378749   \n",
       "2    0.001241  0.000537  0.000703  ...  0.452232  0.603910   1.479533   \n",
       "3    0.005249  0.002413  0.002591  ...  4.925777  0.588674  24.452608   \n",
       "4    0.001894  0.001231  0.001088  ...  0.774701  0.889606  15.672068   \n",
       "..        ...       ...       ...  ...       ...       ...        ...   \n",
       "895  0.000399  0.000657  0.000297  ...  0.002010  0.003282   0.014107   \n",
       "896  0.000094  0.000065  0.000078  ...  0.000989  0.001065   0.005478   \n",
       "897  0.000034  0.000052  0.000053  ...  0.000620  0.000888   0.005123   \n",
       "898  0.000006  0.000011  0.000007  ...  0.000186  0.000297   0.002047   \n",
       "899  0.000108  0.000061  0.000094  ...  0.005562  0.003180   0.022594   \n",
       "\n",
       "            30        31        32         33         34        35    label  \n",
       "0     8.202133  0.974210  2.181164  15.391540  11.462402  0.016049  English  \n",
       "1     5.564082  3.520760  6.403028   7.948945  11.421647  0.014899  English  \n",
       "2     1.281132  2.430287  5.495424   8.274411  11.088083  0.014856  English  \n",
       "3    27.830118  0.291790  0.230298   0.284989   9.626831  0.011871  English  \n",
       "4     3.751125  0.687353  1.843793   5.087078   9.285885  0.011261  English  \n",
       "..         ...       ...       ...        ...        ...       ...      ...  \n",
       "895   0.009643  0.000996  0.003580   0.027846  29.929813  0.032924  Spanish  \n",
       "896   0.008104  0.001649  0.005327   0.012645   9.986907  0.005506  Spanish  \n",
       "897   0.008245  0.002862  0.003984   0.002771   9.939436  0.006557  Spanish  \n",
       "898   0.002857  0.001005  0.003023   0.003097   9.965343  0.004320  Spanish  \n",
       "899   0.009479  0.001067  0.002599   0.014489  30.011419  0.019132  Spanish  \n",
       "\n",
       "[900 rows x 37 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List = []\n",
    "\n",
    "for i in range(0,len(bp)):\n",
    "    if bp[i] == 1:\n",
    "        List.append(i)\n",
    "        \n",
    "df_train = pd.read_csv(\"VoxForge512.csv\")\n",
    "y = (df_train['label'])\n",
    "df_train1 = df_train[df_train.columns[List]]\n",
    "l1 = []\n",
    "for i in range(0,len(List)):\n",
    "    l1.append(i)\n",
    "    \n",
    "df_train1.columns = l1\n",
    "\n",
    "df_train1['label'] = y\n",
    "df_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1.to_csv('VF_BNMRA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
