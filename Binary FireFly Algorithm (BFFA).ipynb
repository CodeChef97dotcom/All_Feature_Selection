{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import combinations as cb\n",
    "import math\n",
    "from copy import deepcopy as dc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate Function \"\"\"\n",
    "class Evaluate:\n",
    "    def __init__(self):\n",
    "        None\n",
    "    def evaluate(self,gen):\n",
    "        None\n",
    "    def check_dimentions(self,dim):\n",
    "        None\n",
    "\n",
    "\"\"\"Common Function\"\"\"\n",
    "def random_search(n,dim):\n",
    "    \"\"\"\n",
    "    create genes list\n",
    "    input:{ n: Number of population, default=20\n",
    "            dim: Number of dimension\n",
    "    }\n",
    "    output:{genes_list → [[0,0,0,1,1,0,1,...]...n]\n",
    "    }\n",
    "    \"\"\"\n",
    "    gens=[[0 for g in range(dim)] for _ in range(n)]\n",
    "    for i,gen in enumerate(gens) :\n",
    "        r=random.randint(1,dim)\n",
    "        for _r in range(r):\n",
    "            gen[_r]=1\n",
    "        random.shuffle(gen)\n",
    "    return gens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BFFA\"\"\"\n",
    "def exchange_binary(binary,score):#,alpha,beta,gamma,r):\n",
    "\n",
    "    #binary in list\n",
    "    al_binary=binary\n",
    "    #movement=move(b,alpha,beta,gamma,r)\n",
    "    movement=math.tanh(score)\n",
    "    ##al_binary=[case7(b) if random.uniform(0,1) < movement else case8(b) for b in binary]\n",
    "    if random.uniform(0,1) < movement:\n",
    "        for i,b in enumerate(binary):\n",
    "            al_binary[i]=case7(b)\n",
    "    else:\n",
    "        for i,b in enumerate(binary):\n",
    "            al_binary[i]=case8(b)\n",
    "    return al_binary\n",
    "\n",
    "def case7(one_bin):\n",
    "    return 1 if random.uniform(-0.1,0.9)<math.tanh(one_bin) else 0\n",
    "def case8(one_bin):\n",
    "    if random.uniform(-0.1,0.9)<math.tanh(int(one_bin)):\n",
    "        if one_bin==1:\n",
    "            return 0\n",
    "        else:return 1\n",
    "    else:return one_bin\n",
    "def case9(one_bin,best):\n",
    "    if random.uniform(0,1)<math.tanh(int(one_bin)):\n",
    "        return best\n",
    "    else:return 0\n",
    "\n",
    "def BFFA(Eval_Func,n=20,m_i=25,minf=0,dim=None,prog=False,gamma=1.0,beta=0.20,alpha=0.25,mp=None):\n",
    "    \"\"\"\n",
    "    input:{ Eval_Func: Evaluate_Function, type is class\n",
    "            n: Number of population, default=20\n",
    "            m_i: Number of max iteration, default=300\n",
    "            minf: minimazation flag, default=0, 0=maximization, 1=minimazation\n",
    "            dim: Number of feature, default=None\n",
    "            prog: Do you want to use a progress bar?, default=False\n",
    "            }\n",
    "    output:{Best value: type float 0.967\n",
    "            Best position: type list(int) [1,0,0,1,.....]\n",
    "            Nunber of 1s in best position: type int [0,1,1,0,1] → 3\n",
    "            }\n",
    "    \"\"\"\n",
    "    estimate=Eval_Func().evaluate\n",
    "    if dim==None:\n",
    "        dim=Eval_Func().check_dimentions(dim)\n",
    "    #flag=dr\n",
    "    global_best=float(\"-inf\") if minf == 0 else float(\"inf\")\n",
    "    pb=float(\"-inf\") if minf == 0 else float(\"inf\")\n",
    "\n",
    "    global_position=tuple([0]*dim)\n",
    "    gen=tuple([0]*dim)\n",
    "    #gamma=1.0\n",
    "    #beta=0.20\n",
    "    #alpha=0.25\n",
    "    gens_dict = {tuple([0]*dim):float(\"-inf\") if minf == 0 else float(\"inf\")}\n",
    "    #gens_dict[global_position]=0.001\n",
    "    gens=random_search(n,dim)\n",
    "    #vs = [[random.choice([0,1]) for i in range(length)] for i in range(N)]\n",
    "    if mp!=None:\n",
    "        with Pool(mp) as p:\n",
    "            alter_fit = p.map(estimate,[gens[i] for i in alter_gens])\n",
    "        for i,j in zip(alter_fit,gens):\n",
    "            gens_dict[tuple(j)]=i\n",
    "            fit[i]=i\n",
    "        global_best=max(fit)\n",
    "        global_position=gens[fit.index(max(fit))]\n",
    "\n",
    "    else:\n",
    "        for gen in gens:\n",
    "            if tuple(gen) in gens_dict:\n",
    "                score = gens_dict[tuple(gen)]\n",
    "            else:\n",
    "                score=estimate(gen)\n",
    "                gens_dict[tuple(gen)]=score\n",
    "            if score > global_best:\n",
    "                global_best=score\n",
    "                global_position=dc(gen)\n",
    "    if prog:\n",
    "        miter=tqdm(range(m_i))\n",
    "    else:\n",
    "        miter=range(m_i)\n",
    "    for it in miter:\n",
    "        for i,x in enumerate(gens):\n",
    "            if mp != None:\n",
    "                for j,y in enumerate(gens):\n",
    "                    if gens_dict[tuple(y)] < gens_dict[tuple(x)]:\n",
    "                        gens[j]=exchange_binary(y,gens_dict[tuple(y)])\n",
    "                    gen = gens[j]\n",
    "\n",
    "                fit=[gens_dict[tuple(g)]  if tuple(g) in gens_dict  else(float(\"-inf\") if minf == 0 else float(\"inf\")) for g in gens]\n",
    "                alter_gens=[i for i,g in enumerate(gens) if tuple(g) not in gens_dict]\n",
    "                #print(len(alter_gens))\n",
    "                with Pool(mp) as p:\n",
    "                    alter_fit = p.map(estimate,[gens[i] for i in alter_gens])\n",
    "                z=0\n",
    "                for k in range(len(fit)):\n",
    "                    if k in alter_gens:\n",
    "                        fit[k]=alter_fit[z]\n",
    "                        gens_dict[tuple(gens[k])]=alter_fit[z]\n",
    "                    else:pass\n",
    "                    if fit[k] > global_best if minf==0 else fit[k] < global_best:\n",
    "                        global_best=dc(fit[k])\n",
    "                        global_position=dc(gen)\n",
    "\n",
    "            else:\n",
    "                for j,y in enumerate(gens):\n",
    "                    if gens_dict[tuple(y)] < gens_dict[tuple(x)]:\n",
    "                        gens[j]=exchange_binary(y,gens_dict[tuple(y)])\n",
    "                    gen = gens[j]\n",
    "                    if tuple(gen) in gens_dict:\n",
    "                        score = gens_dict[tuple(gen)]\n",
    "                    else:\n",
    "                        score=estimate(gens[j])\n",
    "                        gens_dict[tuple(gen)]=score\n",
    "                    if score > global_best if minf==0 else score < global_best:\n",
    "                        global_best=dc(score)\n",
    "                        global_position=dc(gen)\n",
    "    print(len(gens_dict))\n",
    "    return global_best,global_position,global_position.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 129\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    " \n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm\n",
    " \n",
    "SEED = 2018\n",
    "np.random.seed(SEED)\n",
    "df_train = pd.read_csv(\"CSS10_Spec128.csv\")\n",
    "X = np.array(df_train.drop(['label'],axis=1))\n",
    "y = np.array(df_train['label'])\n",
    "from sklearn.model_selection import train_test_split\n",
    "df=df_train\n",
    "(a,b)=np.shape(df)\n",
    "print(a,b)\n",
    "data = df.values[:,0:b-1]\n",
    "label = df.values[:,b-1]\n",
    "# trainX=data\n",
    "# trainy=label\n",
    "cross=4\n",
    "test_size=(1/cross)\n",
    "trainX, testX, trainy, testy = train_test_split(data, label,stratify=label ,test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    from sklearn import svm\n",
    "    from time import time\n",
    "\n",
    "    np.random.seed(20)\n",
    "    tr_d=trainX\n",
    "    te_d=testX\n",
    "    tr_l=trainy\n",
    "    te_l=testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate:#setting class\n",
    "        def __init__(self):#set train_data,label,test_data,label\n",
    "            self.train_l=tr_l\n",
    "            self.train_d=tr_d\n",
    "            self.test_l=te_l\n",
    "            self.test_d=te_d\n",
    "        def evaluate(self,gen):\n",
    "            \"\"\"\n",
    "            Setting of evaluation function.\n",
    "            Here, the correct answer rate is used.\n",
    "              anser_label/all_label\n",
    "            \"\"\"\n",
    "            mask=np.array(gen) > 0\n",
    "            al_data=np.array([al[mask] for al in self.train_d])\n",
    "            al_test_data=np.array([al[mask] for al in self.test_d])\n",
    "            #↑masking with [01]sequence list\n",
    "            res=RandomForestClassifier(n_jobs = -1, verbose = 0, n_estimators=5, criterion='entropy', random_state = 0).fit(al_data,self.train_l).predict(al_test_data)\n",
    "            return np.count_nonzero(self.test_l==res)/len(self.test_l)\n",
    "            #↑evaluate with fittness function\n",
    "        def check_dimentions(self,dim):#check number of all feature\n",
    "            if dim==None:\n",
    "                return len(self.train_d[0])\n",
    "            else:\n",
    "                return dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "CPU times: user 343 ms, sys: 12.3 ms, total: 355 ms\n",
      "Wall time: 431 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s,g,l=BFFA(Eval_Func=Evaluate, n=1, m_i=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.016048</td>\n",
       "      <td>0.045006</td>\n",
       "      <td>0.088202</td>\n",
       "      <td>0.050937</td>\n",
       "      <td>0.048269</td>\n",
       "      <td>0.081694</td>\n",
       "      <td>0.061398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040290</td>\n",
       "      <td>0.035810</td>\n",
       "      <td>0.036298</td>\n",
       "      <td>0.038804</td>\n",
       "      <td>0.033180</td>\n",
       "      <td>0.036938</td>\n",
       "      <td>0.021571</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>6.623209e-03</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>0.005581</td>\n",
       "      <td>0.026885</td>\n",
       "      <td>0.217670</td>\n",
       "      <td>0.280215</td>\n",
       "      <td>0.095449</td>\n",
       "      <td>0.109636</td>\n",
       "      <td>0.252080</td>\n",
       "      <td>0.268016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205824</td>\n",
       "      <td>0.407305</td>\n",
       "      <td>0.610883</td>\n",
       "      <td>0.180494</td>\n",
       "      <td>0.089081</td>\n",
       "      <td>0.081824</td>\n",
       "      <td>0.057899</td>\n",
       "      <td>0.032875</td>\n",
       "      <td>1.317016e-02</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.050511</td>\n",
       "      <td>0.033339</td>\n",
       "      <td>0.020180</td>\n",
       "      <td>0.046618</td>\n",
       "      <td>0.193343</td>\n",
       "      <td>0.242709</td>\n",
       "      <td>0.097945</td>\n",
       "      <td>0.239527</td>\n",
       "      <td>0.509217</td>\n",
       "      <td>0.336884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026522</td>\n",
       "      <td>0.028574</td>\n",
       "      <td>0.018005</td>\n",
       "      <td>0.016250</td>\n",
       "      <td>0.012089</td>\n",
       "      <td>0.009391</td>\n",
       "      <td>0.008720</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>4.109685e-03</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>0.041863</td>\n",
       "      <td>0.200739</td>\n",
       "      <td>0.168802</td>\n",
       "      <td>0.106558</td>\n",
       "      <td>0.527622</td>\n",
       "      <td>0.437945</td>\n",
       "      <td>0.274857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023716</td>\n",
       "      <td>0.014288</td>\n",
       "      <td>0.017838</td>\n",
       "      <td>0.018208</td>\n",
       "      <td>0.014407</td>\n",
       "      <td>0.010028</td>\n",
       "      <td>0.007528</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>4.462213e-03</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>0.027608</td>\n",
       "      <td>0.163129</td>\n",
       "      <td>0.289433</td>\n",
       "      <td>0.170620</td>\n",
       "      <td>0.243592</td>\n",
       "      <td>0.555679</td>\n",
       "      <td>0.161425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209389</td>\n",
       "      <td>0.256350</td>\n",
       "      <td>0.212744</td>\n",
       "      <td>0.145164</td>\n",
       "      <td>0.106921</td>\n",
       "      <td>0.067407</td>\n",
       "      <td>0.049716</td>\n",
       "      <td>0.025963</td>\n",
       "      <td>1.462485e-02</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>0.057926</td>\n",
       "      <td>2.879856</td>\n",
       "      <td>20.679771</td>\n",
       "      <td>20.195250</td>\n",
       "      <td>0.379650</td>\n",
       "      <td>1.958890</td>\n",
       "      <td>2.496896</td>\n",
       "      <td>5.655553</td>\n",
       "      <td>6.943397</td>\n",
       "      <td>5.476128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008678</td>\n",
       "      <td>0.006215</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.014670</td>\n",
       "      <td>0.011008</td>\n",
       "      <td>0.007485</td>\n",
       "      <td>0.007760</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>2.610000e-07</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>0.078999</td>\n",
       "      <td>4.854642</td>\n",
       "      <td>16.547379</td>\n",
       "      <td>10.877720</td>\n",
       "      <td>0.576475</td>\n",
       "      <td>3.127383</td>\n",
       "      <td>4.036949</td>\n",
       "      <td>5.273048</td>\n",
       "      <td>4.024424</td>\n",
       "      <td>6.352676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007153</td>\n",
       "      <td>0.005911</td>\n",
       "      <td>0.006234</td>\n",
       "      <td>0.005297</td>\n",
       "      <td>0.008319</td>\n",
       "      <td>0.005898</td>\n",
       "      <td>0.005996</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>2.550000e-07</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>0.236640</td>\n",
       "      <td>3.645541</td>\n",
       "      <td>11.302727</td>\n",
       "      <td>16.783052</td>\n",
       "      <td>0.851596</td>\n",
       "      <td>2.084428</td>\n",
       "      <td>4.140211</td>\n",
       "      <td>3.063443</td>\n",
       "      <td>2.992726</td>\n",
       "      <td>4.451303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017653</td>\n",
       "      <td>0.019413</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.002933</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>7.720000e-08</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>0.118839</td>\n",
       "      <td>2.011996</td>\n",
       "      <td>15.797884</td>\n",
       "      <td>18.321749</td>\n",
       "      <td>0.504213</td>\n",
       "      <td>1.365581</td>\n",
       "      <td>4.591431</td>\n",
       "      <td>4.060121</td>\n",
       "      <td>7.282707</td>\n",
       "      <td>6.573135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004784</td>\n",
       "      <td>0.004022</td>\n",
       "      <td>0.008535</td>\n",
       "      <td>0.008248</td>\n",
       "      <td>0.010245</td>\n",
       "      <td>0.007024</td>\n",
       "      <td>0.006153</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>1.490000e-07</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>0.032063</td>\n",
       "      <td>1.281586</td>\n",
       "      <td>12.762025</td>\n",
       "      <td>19.568947</td>\n",
       "      <td>0.372410</td>\n",
       "      <td>1.176285</td>\n",
       "      <td>3.092071</td>\n",
       "      <td>5.371708</td>\n",
       "      <td>11.361115</td>\n",
       "      <td>6.919943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.004806</td>\n",
       "      <td>0.011886</td>\n",
       "      <td>0.013311</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.005175</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>6.840000e-08</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1          2          3         4         5         6  \\\n",
       "0     0.000725  0.000390   0.001063   0.016048  0.045006  0.088202  0.050937   \n",
       "1     0.005059  0.002901   0.005581   0.026885  0.217670  0.280215  0.095449   \n",
       "2     0.050511  0.033339   0.020180   0.046618  0.193343  0.242709  0.097945   \n",
       "3     0.001279  0.000901   0.003375   0.041863  0.200739  0.168802  0.106558   \n",
       "4     0.001710  0.001331   0.002648   0.027608  0.163129  0.289433  0.170620   \n",
       "...        ...       ...        ...        ...       ...       ...       ...   \n",
       "6995  0.057926  2.879856  20.679771  20.195250  0.379650  1.958890  2.496896   \n",
       "6996  0.078999  4.854642  16.547379  10.877720  0.576475  3.127383  4.036949   \n",
       "6997  0.236640  3.645541  11.302727  16.783052  0.851596  2.084428  4.140211   \n",
       "6998  0.118839  2.011996  15.797884  18.321749  0.504213  1.365581  4.591431   \n",
       "6999  0.032063  1.281586  12.762025  19.568947  0.372410  1.176285  3.092071   \n",
       "\n",
       "             7          8         9  ...        50        51        52  \\\n",
       "0     0.048269   0.081694  0.061398  ...  0.040290  0.035810  0.036298   \n",
       "1     0.109636   0.252080  0.268016  ...  0.205824  0.407305  0.610883   \n",
       "2     0.239527   0.509217  0.336884  ...  0.026522  0.028574  0.018005   \n",
       "3     0.527622   0.437945  0.274857  ...  0.023716  0.014288  0.017838   \n",
       "4     0.243592   0.555679  0.161425  ...  0.209389  0.256350  0.212744   \n",
       "...        ...        ...       ...  ...       ...       ...       ...   \n",
       "6995  5.655553   6.943397  5.476128  ...  0.008678  0.006215  0.011819   \n",
       "6996  5.273048   4.024424  6.352676  ...  0.007153  0.005911  0.006234   \n",
       "6997  3.063443   2.992726  4.451303  ...  0.017653  0.019413  0.007211   \n",
       "6998  4.060121   7.282707  6.573135  ...  0.004784  0.004022  0.008535   \n",
       "6999  5.371708  11.361115  6.919943  ...  0.004511  0.004806  0.011886   \n",
       "\n",
       "            53        54        55        56        57            58    label  \n",
       "0     0.038804  0.033180  0.036938  0.021571  0.009373  6.623209e-03  Chinese  \n",
       "1     0.180494  0.089081  0.081824  0.057899  0.032875  1.317016e-02  Chinese  \n",
       "2     0.016250  0.012089  0.009391  0.008720  0.006136  4.109685e-03  Chinese  \n",
       "3     0.018208  0.014407  0.010028  0.007528  0.005653  4.462213e-03  Chinese  \n",
       "4     0.145164  0.106921  0.067407  0.049716  0.025963  1.462485e-02  Chinese  \n",
       "...        ...       ...       ...       ...       ...           ...      ...  \n",
       "6995  0.014670  0.011008  0.007485  0.007760  0.001036  2.610000e-07  Spanish  \n",
       "6996  0.005297  0.008319  0.005898  0.005996  0.001108  2.550000e-07  Spanish  \n",
       "6997  0.006127  0.004149  0.002933  0.002529  0.000331  7.720000e-08  Spanish  \n",
       "6998  0.008248  0.010245  0.007024  0.006153  0.000721  1.490000e-07  Spanish  \n",
       "6999  0.013311  0.006864  0.005175  0.003795  0.000411  6.840000e-08  Spanish  \n",
       "\n",
       "[7000 rows x 60 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List = []\n",
    "\n",
    "for i in range(0,len(g)):\n",
    "    if g[i] == 1:\n",
    "        List.append(i)\n",
    "        \n",
    "df_train = pd.read_csv(\"CSS10_Spec128.csv\")\n",
    "y = (df_train['label'])\n",
    "df_train1 = df_train[df_train.columns[List]]\n",
    "l1 = []\n",
    "for i in range(0,len(List)):\n",
    "    l1.append(i)\n",
    "    \n",
    "df_train1.columns = l1\n",
    "\n",
    "df_train1['label'] = y\n",
    "df_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1.to_csv('BFFA128.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
